===============================================================================
2024-12-03 First steps with ansible

Copied terraform files from my aws-salt module, and cut them way down to
create a single instance with a security group that allows in ssh, http, and
ping from outside.

Installed ansible with 
sudo apt install ansible-core -y

In my terraform directory created a make target "inventory" that builds the
inventory file in ansible/inventory.ini

So the following work in the ansible directory:
ansible-inventory -i inventory.ini --list
ansible --user ubuntu myhosts -m ping -i inventory.ini

I created a simple playbook (from the Ansible documentation site):
--- playbook.yml
- name: My first play
  hosts: myhosts
  tasks:
   - name: Ping my hosts
     ansible.builtin.ping:

   - name: Print message
     ansible.builtin.debug:
       msg: Hello world
---

Then this works as advertised in the tutorial:
ansible-playbook --user ubuntu -i inventory.ini playbook.yaml

Installed the nginx collection:
ansible-galaxy collection install nginxinc.nginx_core

Then run the playbook to create a default nginx server:
ansible-playbook --user ubuntu -i inventory.ini ~/.ansible/collections/ansible_collections/nginxinc/nginx_core/playbooks/deploy-nginx-web-server.yml

This gets through all the testing and verification steps, but errors out with
a permissions error on the actual apt install portion.
---
fatal: [3.81.108.168]: FAILED! => {"changed": false, "msg": "Failed to lock apt for exclusive operation: Failed to lock directory /var/lib/apt/lists/: E:Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)"}
---

I think I have to tell it to use sudo (using the "become" method).

Yeah, that was the problem.  This works:
ansible-playbook --become --user ubuntu -i inventory.ini ~/.ansible/collections/ansible_collections/nginxinc/nginx_core/playbooks/deploy-nginx-web-server.yml

And issuing a curl to that address returns the default nginx web page.

This is already a LOT easier than dealing with SaltStack.  We will see what
other hurdles we find with Ansible though.

===============================================================================
2024-12-03-2 Seed of idea

The idea that I had comes from a few things about Ansible:
- Agentless architecture (I think I can also do this with salt with salt-ssh?)
- Concept of Execution Environments - essentially compiling a container with the Ansible controller and the configuration all bundled up
- The fact that Ansible has terraform-like features for manipulating AWS

What this leads to is a 2-step process:
1.  Create the AWS infrastructure that you want to manage
2.  Create an Ansible Execution Environment (I call it a "seed") to manage that infrastructure, and deploy it to ECS to run persistently.

You could even combine these, but then you have to give the container running
on ECS permissions to create AWS resources, which is probably not what you
want.  But that could be an option for the future.

Anyway, I will prototype this with step 1 still being done with Terraform
(because I am familiar with that), then build a "seed", and deploy that to ECS
to then manage the created resources.  This way I don't have to create an EC2
instance to act as the configuration management system controller, like I did
with aws-salt.

===============================================================================
2024-12-06 Design notes

Doing some of the ansible labs on acloud.guru.  

Looks like I can use an inventory plugin to grab hosts based on EC2 instance
tags (or a lot of other criteria).

List of inventory plugins:
ansible-doc -t inventory -l

Docs for ec2 plugin:
ansible-doc -t inventory amazon.aws.aws_ec2

With the idea of having an ECS container running ansible, would need the
parameter "assume_role_arn" with a role listed that can list information about
EC2 instances.

Can use "compose" options to use jinja2 expressions to compose variables.

Would want to use an include_filters key to only include EC2 instances that
have an "ansible_inventory" or "cluster_type" or something tag defined.

Looks like you define key/secret key in environment variables.  This only
matters if doing it from my machine (I guess it doesn't pick up from aws
cli?).  If it is running as an EC2 container then what matters is the IAM
permissions/role for the container.

Can also leverage ssm with "use_ssm_inventory".  I guess the question is if I
should stash information in tags to the instances, or upload ssm keys.  Works
both ways I guess.


